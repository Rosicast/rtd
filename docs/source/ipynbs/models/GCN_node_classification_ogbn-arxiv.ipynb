{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf47527b",
   "metadata": {},
   "source": [
    "# GCN\n",
    "\n",
    "Node Property Prediction\n",
    "\n",
    "In this section we will build our first graph neural network using PyTorch Geometric. Then we will apply it to the task of node property prediction (node classification).\n",
    "\n",
    "Specifically, we will use GCN as the foundation for your graph neural network ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). To do so, we will work with PyG's built-in `GCNConv` layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4cc70f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35de41d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd854f",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06648583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  dataset_name = 'ogbn-arxiv'\n",
    "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "  data = dataset[0]\n",
    "\n",
    "  # Make the adjacency matrix to symmetric\n",
    "  data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "  # If you use GPU, the device should be cuda\n",
    "  print('Device: {}'.format(device))\n",
    "\n",
    "  data = data.to(device)\n",
    "  split_idx = dataset.get_idx_split()\n",
    "  train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af642a9",
   "metadata": {},
   "source": [
    "## GCN Model\n",
    "\n",
    "Now we will implement our GCN model!\n",
    "\n",
    "Please follow the figure below to implement the `forward` function.\n",
    "\n",
    "![img](../../images/cs224w-colab2-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f05670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False):\n",
    "        # TODO: Implement this function that initializes self.convs, \n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
    "        ## 'out_channels'. More information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        ## More information please refer to the documentation: \n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "        \n",
    "        # 1. use torch.nn.ModuleList for self.convs\n",
    "        # 2. self.convs has num_layers GCNConv layers\n",
    "        # 5. use 'in_channels' and 'out_channels'\n",
    "        self.convs = torch.nn.ModuleList(\n",
    "            [GCNConv(in_channels=input_dim, out_channels=hidden_dim)] +\n",
    "            [GCNConv(in_channels=hidden_dim, out_channels=hidden_dim) for i in range(num_layers-2)] + \n",
    "            [GCNConv(in_channels=hidden_dim, out_channels=output_dim)]\n",
    "            )\n",
    "        \n",
    "        # 1. use torch.nn.ModuleList for self.bns\n",
    "        # 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        # 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
    "        self.bns = torch.nn.ModuleList(\n",
    "            [torch.nn.BatchNorm1d(num_features=hidden_dim) for i in range(num_layers-1)]\n",
    "            )\n",
    "\n",
    "        # 4. use torch.nn.LogSoftmax for self.softmax\n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element to be zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        # TODO: Implement this function that takes the feature tensor x,\n",
    "        # edge_index tensor adj_t and returns the output tensor as\n",
    "        # shown in the figure.\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as showing in the figure\n",
    "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## More information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 3. Don't forget to set F.dropout training to self.training\n",
    "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "        \n",
    "        # 2. use torch.nn.functional.relu and torch.nn.functional.dropout\n",
    "        # 3. set F.dropout training to self.training\n",
    "        # from the first layer to n-1 layer\n",
    "        for conv, bn in zip(self.convs[:-1], self.bns):\n",
    "            x_f = torch.nn.functional.relu(bn(conv(x, adj_t)))\n",
    "            if self.training:\n",
    "                x_f = torch.nn.functional.dropout(x_f, p=self.dropout)\n",
    "            x = x_f\n",
    "        \n",
    "        # 4. If return_embeds is True, then skip the last softmax layer\n",
    "        # the last layer\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        out = x if self.return_embeds else self.softmax(x)\n",
    "\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7433c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement this function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slicing the model output and label by train_idx\n",
    "    ## 4. Feed the sliced output and label to loss_fn\n",
    "    ## (~4 lines of code)\n",
    "\n",
    "    # 1. Zero grad the optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 2. Feed the data into the model\n",
    "    out = model(data.x, data.adj_t)\n",
    "\n",
    "    # 3. Slicing the model output and label by train_idx\n",
    "    sliced_output, sliced_label = out[train_idx], data.y[train_idx].reshape(-1)\n",
    "\n",
    "    # 4. Feed the sliced output and label to loss_fn\n",
    "    loss = loss_fn(sliced_output, sliced_label)\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fcaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    # TODO: Implement this function that tests the model by \n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = None\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    ## Note:\n",
    "    ## 1. No index slicing here\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5375fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "  }\n",
    "  args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d83a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(data.num_features, args['hidden_dim'],\n",
    "            dataset.num_classes, args['num_layers'],\n",
    "            args['dropout']).to(device)\n",
    "evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8deab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gnn\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 4.0346, Train: 29.90%, Valid: 32.52% Test: 30.71%\n",
      "Epoch: 02, Loss: 2.3318, Train: 24.67%, Valid: 22.16% Test: 27.45%\n",
      "Epoch: 03, Loss: 1.9201, Train: 32.05%, Valid: 27.90% Test: 32.16%\n",
      "Epoch: 04, Loss: 1.7597, Train: 37.44%, Valid: 34.33% Test: 35.43%\n",
      "Epoch: 05, Loss: 1.6729, Train: 34.51%, Valid: 30.50% Test: 27.29%\n",
      "Epoch: 06, Loss: 1.5858, Train: 35.09%, Valid: 32.65% Test: 29.51%\n",
      "Epoch: 07, Loss: 1.5176, Train: 36.10%, Valid: 34.45% Test: 31.95%\n",
      "Epoch: 08, Loss: 1.4656, Train: 37.75%, Valid: 36.59% Test: 35.23%\n",
      "Epoch: 09, Loss: 1.4186, Train: 39.37%, Valid: 39.08% Test: 39.62%\n",
      "Epoch: 10, Loss: 1.3821, Train: 40.02%, Valid: 37.46% Test: 39.68%\n",
      "Epoch: 11, Loss: 1.3509, Train: 39.30%, Valid: 33.63% Test: 35.36%\n",
      "Epoch: 12, Loss: 1.3273, Train: 39.09%, Valid: 31.49% Test: 32.18%\n",
      "Epoch: 13, Loss: 1.3065, Train: 41.37%, Valid: 34.63% Test: 35.20%\n",
      "Epoch: 14, Loss: 1.2826, Train: 45.66%, Valid: 41.70% Test: 43.19%\n",
      "Epoch: 15, Loss: 1.2578, Train: 49.50%, Valid: 48.52% Test: 50.58%\n",
      "Epoch: 16, Loss: 1.2423, Train: 51.93%, Valid: 52.34% Test: 54.49%\n",
      "Epoch: 17, Loss: 1.2293, Train: 53.44%, Valid: 54.21% Test: 56.26%\n",
      "Epoch: 18, Loss: 1.2122, Train: 55.02%, Valid: 56.13% Test: 57.58%\n",
      "Epoch: 19, Loss: 1.1993, Train: 56.16%, Valid: 57.53% Test: 58.74%\n",
      "Epoch: 20, Loss: 1.1868, Train: 57.14%, Valid: 58.59% Test: 59.97%\n",
      "Epoch: 21, Loss: 1.1736, Train: 58.00%, Valid: 59.74% Test: 61.07%\n",
      "Epoch: 22, Loss: 1.1651, Train: 59.11%, Valid: 60.87% Test: 61.98%\n",
      "Epoch: 23, Loss: 1.1510, Train: 60.39%, Valid: 62.16% Test: 62.73%\n",
      "Epoch: 24, Loss: 1.1427, Train: 61.67%, Valid: 63.23% Test: 63.72%\n",
      "Epoch: 25, Loss: 1.1331, Train: 63.23%, Valid: 64.29% Test: 64.53%\n",
      "Epoch: 26, Loss: 1.1208, Train: 64.69%, Valid: 64.79% Test: 64.73%\n",
      "Epoch: 27, Loss: 1.1179, Train: 66.00%, Valid: 65.56% Test: 64.87%\n",
      "Epoch: 28, Loss: 1.1079, Train: 67.00%, Valid: 66.25% Test: 65.31%\n",
      "Epoch: 29, Loss: 1.1002, Train: 67.60%, Valid: 66.83% Test: 66.02%\n",
      "Epoch: 30, Loss: 1.0954, Train: 67.92%, Valid: 67.37% Test: 66.91%\n",
      "Epoch: 31, Loss: 1.0861, Train: 68.10%, Valid: 67.25% Test: 67.29%\n",
      "Epoch: 32, Loss: 1.0792, Train: 68.01%, Valid: 67.08% Test: 67.53%\n",
      "Epoch: 33, Loss: 1.0778, Train: 68.19%, Valid: 67.39% Test: 67.80%\n",
      "Epoch: 34, Loss: 1.0725, Train: 68.60%, Valid: 68.06% Test: 68.09%\n",
      "Epoch: 35, Loss: 1.0662, Train: 68.95%, Valid: 68.61% Test: 68.46%\n",
      "Epoch: 36, Loss: 1.0624, Train: 69.14%, Valid: 68.81% Test: 68.73%\n",
      "Epoch: 37, Loss: 1.0562, Train: 69.22%, Valid: 68.92% Test: 68.75%\n",
      "Epoch: 38, Loss: 1.0517, Train: 69.22%, Valid: 68.83% Test: 68.74%\n",
      "Epoch: 39, Loss: 1.0479, Train: 69.38%, Valid: 68.56% Test: 68.25%\n",
      "Epoch: 40, Loss: 1.0421, Train: 69.57%, Valid: 68.31% Test: 67.59%\n",
      "Epoch: 41, Loss: 1.0387, Train: 69.80%, Valid: 68.56% Test: 67.77%\n",
      "Epoch: 42, Loss: 1.0382, Train: 70.01%, Valid: 69.15% Test: 68.68%\n",
      "Epoch: 43, Loss: 1.0301, Train: 70.22%, Valid: 69.57% Test: 69.47%\n",
      "Epoch: 44, Loss: 1.0247, Train: 70.34%, Valid: 69.88% Test: 69.75%\n",
      "Epoch: 45, Loss: 1.0274, Train: 70.52%, Valid: 70.03% Test: 69.75%\n",
      "Epoch: 46, Loss: 1.0193, Train: 70.89%, Valid: 70.28% Test: 69.58%\n",
      "Epoch: 47, Loss: 1.0181, Train: 71.12%, Valid: 70.36% Test: 69.29%\n",
      "Epoch: 48, Loss: 1.0129, Train: 71.28%, Valid: 70.59% Test: 69.76%\n",
      "Epoch: 49, Loss: 1.0100, Train: 71.38%, Valid: 70.87% Test: 70.41%\n",
      "Epoch: 50, Loss: 1.0108, Train: 71.36%, Valid: 70.86% Test: 70.43%\n",
      "Epoch: 51, Loss: 1.0038, Train: 71.45%, Valid: 70.77% Test: 70.31%\n",
      "Epoch: 52, Loss: 1.0017, Train: 71.58%, Valid: 70.80% Test: 70.03%\n",
      "Epoch: 53, Loss: 0.9999, Train: 71.63%, Valid: 70.83% Test: 69.91%\n",
      "Epoch: 54, Loss: 0.9978, Train: 71.67%, Valid: 70.94% Test: 70.20%\n",
      "Epoch: 55, Loss: 0.9929, Train: 71.67%, Valid: 70.92% Test: 70.63%\n",
      "Epoch: 56, Loss: 0.9915, Train: 71.71%, Valid: 71.00% Test: 70.72%\n",
      "Epoch: 57, Loss: 0.9857, Train: 71.88%, Valid: 71.17% Test: 70.34%\n",
      "Epoch: 58, Loss: 0.9881, Train: 72.04%, Valid: 70.71% Test: 69.24%\n",
      "Epoch: 59, Loss: 0.9838, Train: 71.97%, Valid: 70.22% Test: 68.34%\n",
      "Epoch: 60, Loss: 0.9827, Train: 72.13%, Valid: 70.59% Test: 68.77%\n",
      "Epoch: 61, Loss: 0.9793, Train: 72.24%, Valid: 71.03% Test: 69.90%\n",
      "Epoch: 62, Loss: 0.9751, Train: 72.21%, Valid: 71.13% Test: 70.34%\n",
      "Epoch: 63, Loss: 0.9745, Train: 72.28%, Valid: 70.95% Test: 69.88%\n",
      "Epoch: 64, Loss: 0.9735, Train: 72.34%, Valid: 70.59% Test: 68.97%\n",
      "Epoch: 65, Loss: 0.9713, Train: 72.32%, Valid: 70.58% Test: 68.83%\n",
      "Epoch: 66, Loss: 0.9678, Train: 72.37%, Valid: 70.98% Test: 69.96%\n",
      "Epoch: 67, Loss: 0.9678, Train: 72.47%, Valid: 71.21% Test: 70.48%\n",
      "Epoch: 68, Loss: 0.9659, Train: 72.64%, Valid: 71.23% Test: 70.14%\n",
      "Epoch: 69, Loss: 0.9607, Train: 72.66%, Valid: 70.93% Test: 69.29%\n",
      "Epoch: 70, Loss: 0.9606, Train: 72.67%, Valid: 70.83% Test: 69.22%\n",
      "Epoch: 71, Loss: 0.9544, Train: 72.65%, Valid: 71.06% Test: 69.98%\n",
      "Epoch: 72, Loss: 0.9544, Train: 72.63%, Valid: 71.28% Test: 70.38%\n",
      "Epoch: 73, Loss: 0.9525, Train: 72.87%, Valid: 71.47% Test: 70.52%\n",
      "Epoch: 74, Loss: 0.9505, Train: 73.01%, Valid: 71.40% Test: 70.13%\n",
      "Epoch: 75, Loss: 0.9473, Train: 73.00%, Valid: 71.35% Test: 70.03%\n",
      "Epoch: 76, Loss: 0.9487, Train: 73.02%, Valid: 71.43% Test: 70.16%\n",
      "Epoch: 77, Loss: 0.9469, Train: 73.07%, Valid: 71.36% Test: 70.30%\n",
      "Epoch: 78, Loss: 0.9468, Train: 73.07%, Valid: 71.46% Test: 70.24%\n",
      "Epoch: 79, Loss: 0.9420, Train: 73.12%, Valid: 71.31% Test: 70.14%\n",
      "Epoch: 80, Loss: 0.9408, Train: 73.14%, Valid: 71.27% Test: 70.24%\n",
      "Epoch: 81, Loss: 0.9410, Train: 73.09%, Valid: 71.28% Test: 70.34%\n",
      "Epoch: 82, Loss: 0.9390, Train: 73.17%, Valid: 71.40% Test: 70.41%\n",
      "Epoch: 83, Loss: 0.9380, Train: 73.31%, Valid: 71.67% Test: 70.47%\n",
      "Epoch: 84, Loss: 0.9336, Train: 73.39%, Valid: 71.46% Test: 70.02%\n",
      "Epoch: 85, Loss: 0.9334, Train: 73.48%, Valid: 71.69% Test: 70.21%\n",
      "Epoch: 86, Loss: 0.9305, Train: 73.49%, Valid: 71.51% Test: 70.52%\n",
      "Epoch: 87, Loss: 0.9295, Train: 73.45%, Valid: 71.35% Test: 70.46%\n",
      "Epoch: 88, Loss: 0.9247, Train: 73.54%, Valid: 71.31% Test: 69.91%\n",
      "Epoch: 89, Loss: 0.9279, Train: 73.64%, Valid: 71.10% Test: 69.24%\n",
      "Epoch: 90, Loss: 0.9232, Train: 73.72%, Valid: 71.12% Test: 69.20%\n",
      "Epoch: 91, Loss: 0.9246, Train: 73.84%, Valid: 71.45% Test: 69.85%\n",
      "Epoch: 92, Loss: 0.9209, Train: 73.86%, Valid: 71.45% Test: 70.26%\n",
      "Epoch: 93, Loss: 0.9207, Train: 73.86%, Valid: 71.74% Test: 70.68%\n",
      "Epoch: 94, Loss: 0.9184, Train: 73.93%, Valid: 71.98% Test: 70.87%\n",
      "Epoch: 95, Loss: 0.9168, Train: 73.87%, Valid: 71.88% Test: 70.64%\n",
      "Epoch: 96, Loss: 0.9141, Train: 73.70%, Valid: 71.83% Test: 70.60%\n",
      "Epoch: 97, Loss: 0.9149, Train: 73.58%, Valid: 71.86% Test: 71.15%\n",
      "Epoch: 98, Loss: 0.9123, Train: 73.69%, Valid: 71.84% Test: 70.87%\n",
      "Epoch: 99, Loss: 0.9116, Train: 73.86%, Valid: 71.85% Test: 70.56%\n",
      "Epoch: 100, Loss: 0.9121, Train: 73.86%, Valid: 71.62% Test: 70.58%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  # reset the parameters to initial random value\n",
    "  model.reset_parameters()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "  loss_fn = F.nll_loss\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
    "    result = test(model, data, split_idx, evaluator)\n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * valid_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71b9bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Train: 73.93%, Valid: 71.98% Test: 70.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\gnn\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "best_result = test(best_model, data, split_idx, evaluator)\n",
    "train_acc, valid_acc, test_acc = best_result\n",
    "print(f'Best model: '\n",
    "      f'Train: {100 * train_acc:.2f}%, '\n",
    "      f'Valid: {100 * valid_acc:.2f}% '\n",
    "      f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59728f24",
   "metadata": {},
   "source": [
    "## references\n",
    "\n",
    "cs224w colab 2\n",
    "https://colab.research.google.com/drive/1BRPw3WQjP8ANSFz-4Z1ldtNt9g7zm-bv?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
