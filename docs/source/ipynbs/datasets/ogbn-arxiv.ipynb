{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d45685",
   "metadata": {},
   "source": [
    "# ogbn-arxiv\n",
    "\n",
    "**Graph**: The ogbn-arxiv dataset is a directed graph, representing the citation network between all Computer Science (CS) arXiv papers indexed by MAG [1]. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are computed by running the skip-gram model [2] over the MAG corpus. We also provide the mapping from MAG paper IDs into the raw texts of titles and abstracts here. In addition, all papers are also associated with the year that the corresponding paper was published.\n",
    "\n",
    "**Prediction task**: The task is to predict the 40 subject areas of arXiv CS papers, e.g., cs.AI, cs.LG, and cs.OS, which are manually determined (i.e., labeled) by the paper’s authors and arXiv moderators. With the volume of scientific publications doubling every 12 years over the past century, it is practically important to automatically classify each publication’s areas and topics. Formally, the task is to predict the primary categories of the arXiv papers, which is formulated as a 40-class classification problem.\n",
    "\n",
    "**Dataset splitting**: We consider a realistic data split based on the publication dates of the papers. The general setting is that the ML models are trained on existing papers and then used to predict the subject areas of newly-published papers, which supports the direct application of them into real-world scenarios, such as helping the arXiv moderators. Specifically, we propose to train on papers published until 2017, validate on those published in 2018, and test on those published since 2019.\n",
    "\n",
    "## [Leaderboard](https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bddd5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874b652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b05349",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddfd8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  dataset_name = 'ogbn-arxiv'\n",
    "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
    "                                  transform=T.ToSparseTensor())\n",
    "  data = dataset[0]\n",
    "\n",
    "  # Make the adjacency matrix to symmetric\n",
    "  data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "  split_idx = dataset.get_idx_split()\n",
    "  train_idx = split_idx['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fa7af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([     0,      1,      2,  ..., 169145, 169148, 169251]),\n",
       " 'valid': tensor([   349,    357,    366,  ..., 169185, 169261, 169296]),\n",
       " 'test': tensor([   346,    398,    451,  ..., 169340, 169341, 169342])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2758398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PygNodePropPredDataset():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 128\n",
      "Number of classes: 40\n",
      "==============================================================\n",
      "Number of nodes: 169343\n",
      "Number of node features: 128\n",
      "Number of edges: 2315598\n",
      "Number of edge features: 0\n",
      "Average node degree: 27.35\n",
      "============= split ==========\n",
      "Number of training nodes: 90941\n",
      "Training node label rate: 0.54\n",
      "Number of validation nodes: 29799\n",
      "validation node label rate: 0.18\n",
      "Number of test nodes: 48603\n",
      "test node label rate: 0.29\n",
      "============ properties ===========\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n",
      "Is directed: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of edge features: {data.num_edge_features}')\n",
    "print(f'Average node degree: {(2*data.num_edges) / data.num_nodes:.2f}')\n",
    "\n",
    "print(\"============= split ==========\")\n",
    "\n",
    "print(f\"Number of training nodes: {split_idx['train'].shape[0]}\")\n",
    "print(f\"Training node label rate: {int(split_idx['train'].shape[0]) / data.num_nodes:.2f}\")\n",
    "print(f\"Number of validation nodes: {split_idx['valid'].shape[0]}\")\n",
    "print(f\"validation node label rate: {int(split_idx['valid'].shape[0]) / data.num_nodes:.2f}\")\n",
    "print(f\"Number of test nodes: {split_idx['test'].shape[0]}\")\n",
    "print(f\"test node label rate: {int(split_idx['test'].shape[0]) / data.num_nodes:.2f}\")\n",
    "\n",
    "print(\"============ properties ===========\")\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(f'Is directed: {data.is_directed()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371da3e6",
   "metadata": {},
   "source": [
    "The papar mention that the number of edges are 1166243, however, what the data shows is diffrent as 2315598. Since, the number of edges is changed, other related values also different from the [paper](https://arxiv.org/pdf/2005.00687.pdf])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1692025b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=2315598])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c064d",
   "metadata": {},
   "source": [
    "`x=[169343, 128]`: `[num_nodes, num_node_features]` Each of the 169343 nodes is assigned a 128-dim feature vector\n",
    "\n",
    "`data.num_node_features`: the number of node features. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are computed by running the WORD2VEC model (Mikolovet al., 2013) over the MAG corpus.\n",
    "\n",
    "`node_year`: the year that the corresponding paper was published.\n",
    "\n",
    "`y=[169343, 1]`: a node labels\n",
    "\n",
    "`adj_t`: a adjaceny matrix\n",
    "\n",
    "`nnz`: the number of non-zero entries in the adjacenct matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770721ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0579, -0.0525, -0.0726, -0.0266,  0.1304, -0.2414, -0.4492, -0.0184,\n",
       "        -0.0872,  0.1123, -0.0921, -0.2896, -0.0810,  0.0745, -0.1562, -0.0974,\n",
       "         0.1194,  0.6458,  0.0774, -0.0939, -0.4004,  0.3114, -0.5418,  0.0805,\n",
       "        -0.0069,  0.5423, -0.0122, -0.1808,  0.0165,  0.0508, -0.2083, -0.0870,\n",
       "         0.0124,  0.2817,  0.1004, -0.1643,  0.0269,  0.0782,  0.0795, -0.0134,\n",
       "         0.2915,  0.0416, -0.1414, -0.1345,  0.0162,  0.2810, -0.0919, -0.2403,\n",
       "         0.4618,  0.1873,  0.1533,  0.0331,  0.0108,  0.0124, -0.1589,  0.0980,\n",
       "         0.0305,  0.0162, -0.0957,  0.0521,  0.3218, -0.1057,  0.2229, -0.1206,\n",
       "        -0.1723,  0.3954,  0.0883, -0.2219,  0.2310, -0.2096, -0.1125, -0.0644,\n",
       "         0.0697, -0.1574,  0.0223, -0.4190,  0.1344,  0.2605,  0.0417, -0.0935,\n",
       "        -0.0516, -0.0255,  0.7744,  0.0581,  0.0452,  0.0571, -0.5482, -0.0464,\n",
       "         0.8728,  0.0119,  0.3891, -0.0859,  0.1116,  0.0618,  0.0015,  0.0476,\n",
       "         0.0363,  0.2586,  0.2359, -0.0290, -0.1415,  0.7106, -0.0571, -0.1174,\n",
       "         0.3059,  0.1670, -0.1990,  0.1276,  0.0270,  0.5458, -0.1917, -0.0696,\n",
       "        -0.1111,  0.1142,  0.1162, -0.0159,  0.1159, -0.0624,  0.2115, -0.2261,\n",
       "        -0.1856,  0.0532,  0.3329,  0.1042,  0.0074,  0.1734, -0.1728, -0.1401])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9ec88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8430a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_sparse.tensor.SparseTensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.adj_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e5e7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169343"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.adj_t[0].size(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0263dcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.adj_t[0].nnz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91727366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001718405839036748"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.adj_t[0].density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ca2a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64df8ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2013])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.node_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b931d302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b5d0a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396–413, 2020.\n",
    "\n",
    "[2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representationsof words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NeurIPS), pp. 3111–3119, 2013.\n",
    "\n",
    "cs224w colab 2\n",
    "https://colab.research.google.com/drive/1BRPw3WQjP8ANSFz-4Z1ldtNt9g7zm-bv?usp=sharing\n",
    "\n",
    "https://ogb.stanford.edu/docs/nodeprop/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
